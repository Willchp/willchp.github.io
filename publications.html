<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="initial-scale=1, width=device-width" />

    <link rel="stylesheet" href="./global.css" />
    <link rel="stylesheet" href="./publications.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link
      rel="stylesheet"
      href="https://fonts.googleapis.com/css2?family=IBM Plex Sans:wght@300;400;500;700&display=swap"
    />
    <link
      rel="stylesheet"
      href="https://fonts.googleapis.com/css2?family=Source Code Pro:wght@400&display=swap"
    />
    <link
      rel="stylesheet"
      href="https://fonts.googleapis.com/css2?family=Roboto Slab:wght@400;600&display=swap"
    />
    <link
      rel="stylesheet"
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400&display=swap"
    />
    <link
      rel="stylesheet"
      href="https://fonts.googleapis.com/css2?family=Crimson Pro:wght@700&display=swap"
    />
  </head>
  <body>
    <header class="header">
      <div class="buttons">
        <a class="button" href="./index.html">about me</a>
        <a class="active-button-lg" href="./publications.html">publications</a>
        <a class="active-button-sm" href="./publications.html">papers</a>
        <a class="button" href="./creative.html">creative</a>
        <a class="button" href="./assets/cv.pdf">cv</a>
      </div>
      <div class="horizontal-line"></div>
    </header>
    <div class="page-layout">
        <div class="publication-list">
            <div class="research-card">
                <img src="./images/papers/tvl_splash.png" alt="placeholder" />
                <div class="research-card-text">
                    <div class="paper-title">
                        A Touch, Vision, and Language Dataset for Multimodal Alignment
                    </div>
                    <div class="paper-authors">
                        Max Fu, Gaurav Datta*, Huang Huang*,
                        <span class="will-highlight">Will Panitch*</span>,
                        Jaimyn Drake*, Joseph Ortiz, Mustafa Mukadam, Mike Lambeta,
                        Roberto Calandra, Ken Goldberg
                    </div>
                    <div class="mini-abstract">
                        We design and fabricate hardware for collecting aligned tactile
                        and visual scene observations, then use it to curate a dataset
                        of more than 40,000 vision-touch training pairs. Using a
                        combination of human language labels and pseudolabels, we train
                        an aligned tactile encoder and fine-tune a VLM to perform Q&A and
                        understanding tasks based on touch, vision, and language inputs.
                    </div>
                    <div class="link-row">
                        <div class="conference-block">
                            <div class="conference-text">
                                ICML 2024 - Oral
                            </div>
                        </div>
                        <div class="link-buttons">
                            <a class="link-button-wrapper" href="https://arxiv.org/abs/2402.13232">
                                <div class="paper-icons-text-link">
                                    <i class="fa-solid fa-file-lines"></i>
                                </div>
                            </a>
                            <a class="link-button-wrapper" href="https://tactile-vlm.github.io">
                                <div class="paper-icons-site-link">
                                    <i class="fa-solid fa-globe"></i>
                                </div>
                            </a>
                            <a class="link-button-wrapper" href="https://github.com/Max-Fu/tvl">
                                <div class="paper-icons-code-link">
                                    <i class="fa-solid fa-code"></i>
                                </div>
                            </a>
                            <a class="link-button-wrapper" href="https://huggingface.co/datasets/mlfu7/Touch-Vision-Language-Dataset">
                                <div class="paper-icons-data-link">
                                    <i class="fa-solid fa-database"></i>
                                </div>
                            </a>
                            <a class="link-button-wrapper" href="https://huggingface.co/mlfu7/Touch-Vision-Language-Models">
                                <div class="paper-icons-model-link">
                                    <i class="fa-solid fa-download"></i>
                                </div>
                            </a>
                        </div>
                    </div>
                </div>
            </div>
            <div class="research-card">
                <img src="./images/papers/sufia_pick.gif" alt="placeholder" />
                <div class="research-card-text">
                    <div class="paper-title">
                        SuFIA: Language-Guided Augmented Dexterity for
                        Robotic Surgical Assistants
                    </div>
                    <div class="paper-authors">
                        Masoud Moghani, Lars Doorenbos,
                        <span class="will-highlight">Will Panitch</span>,
                        Sean Huver, Mahdi Azizian, Ken Goldberg, Animesh Garg
                    </div>
                    <div class="mini-abstract">
                        We present SuFIA, the first framework for natural 
                        language-guided augmented dexterity for robotic surgical 
                        assistants. SuFIA incorporates the strong reasoning 
                        capabilities of large language models (LLMs) with 
                        perception modules to implement high-level planning and 
                        low-level robot control for surgical sub-task execution. 
                        This enables a task-independent approach to surgical augmented 
                        dexterity and human-robot teaming.
                    </div>
                    <div class="link-row">
                        <div class="conference-block">
                            <div class="conference-text">
                                IROS 2024
                            </div>
                        </div>
                        <div class="link-buttons">
                            <a class="link-button-wrapper" href="https://arxiv.org/abs/2405.05226">
                                <div class="paper-icons-text-link">
                                    <i class="fa-solid fa-file-lines"></i>
                                </div>
                            </a>
                            <a class="link-button-wrapper" href="https://orbit-surgical.github.io/sufia/">
                                <div class="paper-icons-site-link">
                                    <i class="fa-solid fa-globe"></i>
                                </div>
                            </a>
                            <a class="link-button-wrapper" href="https://www.youtube.com/watch?v=ZfZJd4ENny8&feature=youtu.be">
                                <div class="paper-icons-media-link">
                                    <i class="fa-solid fa-video"></i>
                                </div>
                            </a>
                        </div>
                    </div>
                </div>
            </div>
            <div class="research-card">
                <img src="./images/papers/gasket_gif.gif" alt="placeholder" />
                <div class="research-card-text">
                    <div class="paper-title">
                        Automating Deformable Gasket Assembly
                    </div>
                    <div class="paper-authors">
                        Simeon Adebola*, Tara Sadjadpour*, Karim El-Refai*, 
                        <span class="will-highlight">Will Panitch</span>,
                        Zehan Ma, Roy Lin, Tianshuang Qiu, Shreya Ganti, 
                        Charlotte Le, Jaimyn Drake, Ken Goldberg
                    </div>
                    <div class="mini-abstract">
                        We formalize the gasket assembly task, in which a flexible
                        component is inserted into a rigid channel to create a 
                        tight seal. This task is long horizon, low tolerance, and 
                        contact rich, making it challenging to perform in a robotic 
                        setting. We then present both a learned (using diffusion 
                        policy) and an analytical (using computer vision) method for 
                        autonomously performing this task.
                    </div>
                    <div class="link-row">
                        <div class="conference-block">
                            <div class="conference-text">
                                CASE 2024
                            </div>
                        </div>
                        <div class="link-buttons">
                            <a class="link-button-wrapper" href="https://arxiv.org/abs/2402.13232">
                                <div class="paper-icons-text-link">
                                    <i class="fa-solid fa-file-lines"></i>
                                </div>
                            </a>
                            <a class="link-button-wrapper" href="https://berkeleyautomation.github.io/robot-gasket/">
                                <div class="paper-icons-site-link">
                                    <i class="fa-solid fa-globe"></i>
                                </div>
                            </a>
                            <a class="link-button-wrapper" href="https://github.com/SimeonOA/robot_gasket_assembly">
                                <div class="paper-icons-code-link">
                                    <i class="fa-solid fa-code"></i>
                                </div>
                            </a>
                        </div>
                    </div>
                </div>
            </div>
            <div class="research-card">
                <img src="./images/papers/gasket_gaskets.png" alt="placeholder" />
                <div class="research-card-text">
                    <div class="paper-title">
                        Deformable Gasket Assembly
                    </div>
                    <div class="paper-authors">
                        Simeon Adebola*, Tara Sadjadpour*, Karim El-Refai*, 
                        <span class="will-highlight">Will Panitch</span>,
                        Zehan Ma, Roy Lin, Tianshuang Qiu, Shreya Ganti, 
                        Charlotte Le, Jaimyn Drake, Ken Goldberg
                    </div>
                    <div class="mini-abstract">
                        We briefly introduce the gasket assembly task, a
                        high-contact industrial assembly problem, and
                        discuss a number of potential autonomous approaches.
                    </div>
                    <div class="link-row">
                        <div class="conference-block">
                            <div class="conference-text">
                                ICRA 2024 - RMDO Wksp.
                            </div>
                        </div>
                        <div class="link-buttons">
                            <a class="link-button-wrapper" href="https://deformable-workshop.github.io/icra2024/spotlight/01_10_wdo_adebola_automating.pdf">
                                <div class="paper-icons-text-link">
                                    <i class="fa-solid fa-file-lines"></i>
                                </div>
                            </a>
                            <a class="link-button-wrapper" href="https://www.youtube.com/watch?v=0tfjxucbv2s">
                                <div class="paper-icons-media-link">
                                    <i class="fa-solid fa-video"></i>
                                </div>
                            </a>
                        </div>
                    </div>
                </div>
            </div>
            <div class="research-card">
                <img src="./images/papers/ravsi_insertion.gif" alt="placeholder" />
                <div class="research-card-text">
                    <div class="paper-title">
                        Robot-Assisted Vascular Shunt Insertion with the 
                        dVRK Surgical Robot
                    </div>
                    <div class="paper-authors">
                        Karthik Dharmarajan*
                        <span class="will-highlight">Will Panitch*</span>,
                        Baiyu Shi, Huang Huang, Lawrence Yunliang Chen, 
                        Masoud Moghani, Qinxi Yu, Kush Hari, Thomas Low, 
                        Danyal Fer, Animesh Garg, Ken Goldberg
                    </div>
                    <div class="mini-abstract">
                        We propose an expanded framework for vascular shunt insertion
                        assisted by a commercial robot surgical assistant under 
                        various surgical use cases. We further present a physics-based 
                        simulation environment for shunt insertion built on top of the
                        NVIDIA Isaac-ORBIT simulator and a dataset of insertion
                        trajectories collected using the environment. We then use the
                        framework to demonstrate autonomous vascular shunt insertion
                        with the dVRK robot in a realistic vessel phantom.
                    </div>
                    <div class="link-row">
                        <div class="conference-block">
                            <div class="conference-text">
                                JMRR SI 2023
                            </div>
                        </div>
                        <div class="link-buttons">
                            <a class="link-button-wrapper" href="https://worldscientific.com/doi/epdf/10.1142/S2424905X23400068">
                                <div class="paper-icons-text-link">
                                    <i class="fa-solid fa-file-lines"></i>
                                </div>
                            </a>
                            <a class="link-button-wrapper" href="https://sites.google.com/berkeley.edu/ravsi">
                                <div class="paper-icons-site-link">
                                    <i class="fa-solid fa-globe"></i>
                                </div>
                            </a>
                            <a class="link-button-wrapper" href="https://drive.google.com/drive/folders/1nYQCNTCQLfMt9FSvumKsNTBxySbs1ZfQ">
                                <div class="paper-icons-data-link">
                                    <i class="fa-solid fa-database"></i>
                                </div>
                            </a>
                            <a class="link-button-wrapper" href="https://drive.google.com/file/d/1La3GFN3cUHHYZ8dxnDKXvjgXzdVzd91A/view">
                                <div class="paper-icons-media-link">
                                    <i class="fa-solid fa-video"></i>
                                </div>
                            </a>
                        </div>
                    </div>
                </div>
            </div>
            <div class="research-card">
                <img src="./images/papers/trimodal_pipeline.png" alt="placeholder" />
                <div class="research-card-text">
                    <div class="paper-title">
                        A Trimodal Framework for Robot-Assisted Vascular 
                        Shunt Insertion When a Supervising Surgeon is Local,
                        Remote, or Unavailable
                    </div>
                    <div class="paper-authors">
                        Karthik Dharmarajan*
                        <span class="will-highlight">Will Panitch*</span>,
                        Baiyu Shi, Huang Huang, Lawrence Yunliang Chen, 
                        Thomas Low, Danyal Fer, Ken Goldberg
                    </div>
                    <div class="mini-abstract">
                        We develop a robotic system to autonomously assist in 
                        vascular shunt insertion, a common surgical procedure
                        requiring a surgeon-and-surgical-assistant team
                        performed to temporarily restore blood flow to damaged
                        tissues. We consider three scenarios: (1) a surgeon is
                        available locally; (2) a remote surgeon is available
                        via teleoperation; (3) no surgeon is available. In
                        each scenario, the robot operates in a different mode,
                        either by teleoperation or automation, to perform the
                        missing functions.
                    </div>
                    <div class="link-row">
                        <div class="conference-block">
                            <div class="conference-text">
                                ISMR 2023 - Oral
                            </div>
                        </div>
                        <div class="link-buttons">
                            <a class="link-button-wrapper" href="https://ieeexplore.ieee.org/document/10130195">
                                <div class="paper-icons-text-link">
                                    <i class="fa-solid fa-file-lines"></i>
                                </div>
                            </a>
                        </div>
                    </div>
                </div>
            </div>
            <div class="research-card">
                <img src="./images/papers/micro_optics.png" alt="placeholder" />
                <div class="research-card-text">
                    <div class="paper-title">
                        3-D Localization of Micromanipulators Using Microscopy 
                        for Autonomous Visual Servoing
                    </div>
                    <div class="paper-authors">
                        Ryan Mei,
                        <span class="will-highlight">Will Panitch</span>,
                        Laura Waller
                    </div>
                    <div class="mini-abstract">
                        We rapidly determine the 3-D position of a glass-pipette
                        micromanipulator using color-coded illumination and DPC
                        optical microscopy, then demonstrate the potential for
                        autonomous visual servoing and multi-manipulator systems
                        for highly parallelized cell manipulation using the
                        proporsed localization method as a feedback controller.
                    </div>
                    <div class="link-row">
                        <div class="conference-block">
                            <div class="conference-text">
                                Optica COSI 2022
                            </div>
                        </div>
                        <div class="link-buttons">
                            <a class="link-button-wrapper" href="https://opg.optica.org/abstract.cfm?uri=COSI-2022-JW5C.1">
                                <div class="paper-icons-text-link">
                                    <i class="fa-solid fa-file-lines"></i>
                                </div>
                            </a>
                            <a class="link-button-wrapper" href="https://opg.optica.org/abstract.cfm?uri=COSI-2022-JW5C.1#videoPlayer">
                                <div class="paper-icons-media-link">
                                    <i class="fa-solid fa-video"></i>
                                </div>
                            </a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <div class="publication-list">
            <div class="research-blurb">
                > &nbsp;&nbsp;
                This list contains some of the published, peer-reviewed 
                papers that I've been lucky enough to work on. By far 
                the best part of research is the opportunity to 
                collaborate with and learn from some of the smartest, 
                most interesting people in the world. All of these
                papers would not have been possible without my super
                cool co-authors, and I hope that you check out their
                work as well if you find anything here of interest.
            </div>
            <div class="research-card">
                <img src="./images/papers/stitch_suture.png" alt="placeholder" />
                <div class="research-card-text">
                    <div class="paper-title">
                        STITCH: An Augmented Dexterity Algorithm for Suture ThrowsÂ 
                        Involving Thread Management, Cinching, and Handover
                    </div>
                    <div class="paper-authors">
                        Kush Hari*, Hansoul Kim*,
                        <span class="will-highlight">Will Panitch*</span>,
                        Kishore Srinivas, Vincent Schorp, Karthik Dharmarajan, 
                        Shreya Ganti, Tara Sadjadpour, Ken Goldberg
                    </div>
                    <div class="mini-abstract">
                        We teach a surgical robot to autonomously close wounds in
                        dermal tissue using a simple running suture. To allow for
                        the millimeter-level precision necessary to complete this task,
                        we design a novel visual state-estimation and servoing pipeline
                        using an optical flow-based stereo vision model, learned image
                        segmentation. and RANSAC geometry fitting in point cloud space. 
                        Our system demonstrates the ability to close raised, planar
                        wounds without human intervention.
                    </div>
                    <div class="link-row">
                        <div class="conference-block">
                            <div class="conference-text">
                                ISMR 2024 - Best Paper R.U.
                            </div>
                        </div>
                        <div class="link-buttons">
                            <a class="link-button-wrapper" href="https://arxiv.org/abs/2404.05151">
                                <div class="paper-icons-text-link">
                                    <i class="fa-solid fa-file-lines"></i>
                                </div>
                            </a>
                            <a class="link-button-wrapper" href="https://sites.google.com/berkeley.edu/stitch">
                                <div class="paper-icons-site-link">
                                    <i class="fa-solid fa-globe"></i>
                                </div>
                            </a>
                            <a class="link-button-wrapper" href="https://www.youtube.com/watch?v=Yy7Qj7YDf54">
                                <div class="paper-icons-media-link">
                                    <i class="fa-solid fa-video"></i>
                                </div>
                            </a>
                        </div>
                    </div>
                </div>
            </div>
            <div class="research-card">
                <img src="./images/papers/icil_multi.png" alt="placeholder" />
                <div class="research-card-text">
                    <div class="paper-title">
                        In-Context Imitation Learning by Prompting with
                        Sensorimotor Trajectories
                    </div>
                    <div class="paper-authors">
                        Max Fu, Gaurav Datta*, Huang Huang*,
                        <span class="will-highlight">Will Panitch*</span>,
                        Jaimyn Drake*, Joseph Ortiz, Mustafa Mukadam, Mike Lambeta,
                        Roberto Calandra, Ken Goldberg
                    </div>
                    <div class="mini-abstract">
                        Inspired by language models' strong in-context learning
                        capabilities, where models can answer questions based on
                        similar prompts without further training or fine-tuning,
                        we attempt to cast imitation learning as an in-context 
                        learning problem. In particular, we concatenate different 
                        trajectories of the same task and train a multi-modal 
                        sequence model. At inference time, we prompt the model with 
                        a trajectory on a new task and the model performs the same 
                        task in a different environment configuration from the prompt,
                        and demonstrate that this is sufficient for acquiring many 
                        previously unseen motor tasks.
                    </div>
                    <div class="link-row">
                        <div class="conference-block">
                            <div class="conference-text">
                                Under Review
                            </div>
                        </div>
                        <div class="link-buttons">
                            <!-- <a class="link-button-wrapper" href="https://arxiv.org/abs/2402.13232">
                                <div class="paper-icons-text-link">
                                    <i class="fa-solid fa-file-lines"></i>
                                </div>
                            </a> -->
                        </div>
                    </div>
                </div>
            </div>
            <div class="research-card">
                <img src="./images/papers/orbit_ring.gif" alt="placeholder" />
                <div class="research-card-text">
                    <div class="paper-title">
                        ORBIT-Surgical: An Open-Simulation Framework for
                        Accelerated Learning Environments in Surgical
                        Autonomy
                    </div>
                    <div class="paper-authors">
                        Qinxi Yu, Masoud Moghani, Karthik Dharmarajan, Vincent Schorp,
                        <span class="will-highlight">Will Panitch</span>,
                        Jingzhou Liu, Kush Hari, Huang Huang, Mayank Mittal, 
                        Ken Goldberg, Animesh Garg
                    </div>
                    <div class="mini-abstract">
                        We present ORBIT-Surgical, a physics-based surgical 
                        robot simulation framework with photorealistic 
                        rendering in NVIDIA Omniverse. We provide 14 benchmark 
                        surgical training tasks for the da Vinci Research Kit 
                        (dVRK) and Smart Tissue Autonomous Robot (STAR). 
                        ORBIT-Surgical leverages GPU parallelization to train 
                        reinforcement learning and imitation learning algorithms. 
                        We also demonstrate sim-to-real transfer of policies
                        learned in ORBIT-Surgical onto a physical dVRK robot.
                    </div>
                    <div class="link-row">
                        <div class="conference-block">
                            <div class="conference-text">
                                ICRA 2024
                            </div>
                        </div>
                        <div class="link-buttons">
                            <a class="link-button-wrapper" href="https://arxiv.org/abs/2404.16027">
                                <div class="paper-icons-text-link">
                                    <i class="fa-solid fa-file-lines"></i>
                                </div>
                            </a>
                            <a class="link-button-wrapper" href="https://orbit-surgical.github.io">
                                <div class="paper-icons-site-link">
                                    <i class="fa-solid fa-globe"></i>
                                </div>
                            </a>
                            <a class="link-button-wrapper" href="https://github.com/orbit-surgical/orbit-surgical">
                                <div class="paper-icons-code-link">
                                    <i class="fa-solid fa-code"></i>
                                </div>
                            </a>
                            <a class="link-button-wrapper" href="https://www.youtube.com/watch?v=Jch3N0NqL2k">
                                <div class="paper-icons-media-link">
                                    <i class="fa-solid fa-video"></i>
                                </div>
                            </a>
                        </div>
                    </div>
                </div>
            </div>
            <div class="research-card">
                <img src="./images/papers/tail_splash.png" alt="placeholder" />
                <div class="research-card-text">
                    <div class="paper-title">
                        Self-Supervised Learning for Interactive Perception 
                        of Thread for Autonomous Tail-Shortening
                    </div>
                    <div class="paper-authors">
                        Vincent Schorp,
                        <span class="will-highlight">Will Panitch</span>,
                        Kaushik Shivakumar, Vainavi Viswanath, Justin Kerr,
                        Yahav Avigal, Danyal Fer, Lionel Ott, Ken Goldberg
                    </div>
                    <div class="mini-abstract">
                        We present a machine learning method for tracking
                        and tracing thread in 3D which is robust to
                        occlusions and complex configurations, and apply
                        it to autonomously perform the "tail-shortening"
                        task: pulling thread through an insertion point
                        until a desired "tail" length remains exposed.
                    </div>
                    <div class="link-row">
                        <div class="conference-block">
                            <div class="conference-text">
                                CASE 2023 - Finalist: Best Healthcare Automation Paper
                            </div>
                        </div>
                        <div class="link-buttons">
                            <a class="link-button-wrapper" href="https://arxiv.org/abs/2307.06845">
                                <div class="paper-icons-text-link">
                                    <i class="fa-solid fa-file-lines"></i>
                                </div>
                            </a>
                            <a class="link-button-wrapper" href="https://sites.google.com/berkeley.edu/autolab-surgical-thread/">
                                <div class="paper-icons-site-link">
                                    <i class="fa-solid fa-globe"></i>
                                </div>
                            </a>
                            <a class="link-button-wrapper" href="https://drive.google.com/file/d/1ov4f80cVTer4zLPE7p_W00SFwil5eZSc/view">
                                <div class="paper-icons-media-link">
                                    <i class="fa-solid fa-video"></i>
                                </div>
                            </a>
                        </div>
                    </div>
                </div>
            </div>
            <div class="research-card">
                <img src="./images/papers/avsi_action.jpg" alt="placeholder" />
                <div class="research-card-text">
                    <div class="paper-title">
                        Automating Vascular Shunt Insertion with the 
                        dVRK Surgical Robot
                    </div>
                    <div class="paper-authors">
                        Karthik Dharmarajan*,
                        <span class="will-highlight">Will Panitch*</span>,
                        Muyan Jiang, Kishore Srinivas, Baiyu Shi, 
                        Yahav Avigal, Huang Huang, Thomas Low, Danyal Fer, 
                        Ken Goldberg
                    </div>
                    <div class="mini-abstract">
                        We propose a pipeline to autonomously insert a 
                        rigid object into a similar-diameter flexible tube,
                        as in shunting and deformable assembly tasks.
                        We use a learned visual model and an ensemble of
                        imitation learners to grasp and dilate the flexible
                        rim, then use a chamfer tilt followed by a screw
                        motion to insert the rigid body.
                    </div>
                    <div class="link-row">
                        <div class="conference-block">
                            <div class="conference-text">
                                ICRA 2023
                            </div>
                        </div>
                        <div class="link-buttons">
                            <a class="link-button-wrapper" href="https://arxiv.org/abs/2211.02293">
                                <div class="paper-icons-text-link">
                                    <i class="fa-solid fa-file-lines"></i>
                                </div>
                            </a>
                            <a class="link-button-wrapper" href="https://sites.google.com/berkeley.edu/autolab-avsi">
                                <div class="paper-icons-site-link">
                                    <i class="fa-solid fa-globe"></i>
                                </div>
                            </a>
                        </div>
                    </div>
                </div>
            </div>
            <div class="research-card">
                <img src="./images/papers/sri_splash.jpeg" alt="placeholder" />
                <div class="research-card-text">
                    <div class="paper-title">
                        A Digital Twin Framework for Telesurgery in the
                        Presence of Varying Network Quality
                    </div>
                    <div class="paper-authors">
                        Sophea Bonne*,
                        <span class="will-highlight">Will Panitch*</span>,
                        Karthik Dharmarajan*, Kishore Srinivas*, Jerri-Lynn
                        Kincade, Thomas Low, Bruce Knoth, Cregg Cowan,
                        Dan Fer, Brijen Thananjeyan, Justin Kerr, Jeffrey
                        Ichnowski, Ken Goldberg
                    </div>
                    <div class="mini-abstract">
                        We develop a "digital twin," a 3D simulator that
                        actively mirrors a real environment, for the FLS
                        peg transfer training task, and present a
                        framework that enables a teleoperator to perform
                        this task over unstable or low-bandwidth
                        communication channels using the digital twin.
                        The operator remotely controls the simulated
                        robot, which abstracts their motions into commands
                        and transmits them to the real robot for
                        semi-autonomous execution, then updates the
                        simulator to match the real state of the pegboard.
                    </div>
                    <div class="link-row">
                        <div class="conference-block">
                            <div class="conference-text">
                                CASE 2022
                            </div>
                        </div>
                        <div class="link-buttons">
                            <a class="link-button-wrapper" href="https://ieeexplore.ieee.org/document/9926585">
                                <div class="paper-icons-text-link">
                                    <i class="fa-solid fa-file-lines"></i>
                                </div>
                            </a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <div class="publications-label">
            Publications
        </div>
    </div>
  </body>
</html>
